{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "123f4489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 129350\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  2924835 Jun  5 22:14 test1_1_zeroshot.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  2676315 Jun  5 22:14 test1_2_zeroshot.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  8057361 Jun  5 20:25 test_2_fewshot.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab 25188180 Jun  5 20:24 update_Swissbanl_test1.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  6154207 Jun  5 20:23 update_Swissbanl_test2.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab 25174299 Jun  5 20:24 update_Swissunba_test1.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  6150728 Jun  5 20:24 update_Swissunba_test2.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab 25179866 Jun  5 20:24 update_Unibanl_test1.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  6151792 Jun  5 20:24 update_Unibanl_test2.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab 25183110 Jun  5 20:24 update_Uniunba_test1.csv\r\n",
      "-rw-r--r-- 1 yz92460 ah2lab  6153309 Jun  5 20:24 update_Uniunba_test2.csv\r\n"
     ]
    }
   ],
   "source": [
    "import os, csv\n",
    "os.chdir('/scratch/rcj50704/LOL/DLscript')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "import dataloader as dl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "# plt.rcParams[\"xtick.top\"] = True\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#k-nn\n",
    "from tqdm import tqdm\n",
    "import faiss # Or import annoy\n",
    "! ls -l /work/ah2lab/yuchen/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53c31a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9338014042126379, 0.23345035105315948, 0.3735205616850552)\n"
     ]
    }
   ],
   "source": [
    "files = [\"update_Swissbanl_test1.csv\",\"update_Swissbanl_test2.csv\",\"update_Swissunba_test1.csv\",\"update_Swissunba_test2.csv\",\"update_Unibanl_test1.csv\",\"update_Unibanl_test2.csv\",\"update_Uniunba_test1.csv\",\"update_Uniunba_test2.csv\",]\n",
    "\n",
    "# for file in files:\n",
    "df = pd.read_csv(f\"/work/ah2lab/yuchen/results/test1_2_zeroshot.csv\", usecols=['EC','Predicted EC Number'])\n",
    "true_labels = df['EC'].tolist()\n",
    "predicted_labels = df['Predicted EC Number'].tolist()\n",
    "print(f\"{hiclass_score_metrics(true_labels, predicted_labels)}\")\n",
    "    \n",
    "# df = pd.read_csv(f\"/work/ah2lab/yuchen/results/test_2_fewshot.csv\")\n",
    "# df\n",
    "    \n",
    "# update_Swissbanl_test1.csv = (0.5389131416528676,  0.1347282854132169,  0.21556525666114706)\n",
    "# update_Swissbanl_test2.csv = (0.39608533098746423, 0.09902133274686606, 0.1584341323949857)\n",
    "# update_Swissunba_test1.csv = (0.7512168197099703,  0.18780420492749259, 0.30048672788398817)\n",
    "# update_Swissunba_test2.csv = (0.47371893556190897, 0.11842973389047724, 0.1894875742247636)\n",
    "#     update_Unibanl_test1.csv (0.30979978925184404, 0.07744994731296101, 0.12391991570073761)\n",
    "#     update_Unibanl_test2.csv (0.22850230921486694, 0.057125577303716736,0.09140092368594678)\n",
    "#     update_Uniunba_test1.csv (0.22489838928195094, 0.056224597320487735,0.08995935571278038)\n",
    "#     update_Uniunba_test2.csv (0.21464702001319552, 0.05366175500329888, 0.0858588080052782)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabd151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir=\"/work/ah2lab/vatsal/Data/final_csv/\"\n",
    "def load_df(files):\n",
    "    data_dir = \"/work/ah2lab/yuchen/results\" #\"/work/ah2lab/soumya/\"\n",
    "    file_ptrn = f\"{files}*\"\n",
    "    all_files = glob.glob(f\"{data_dir}/{file_ptrn}\")\n",
    "    print(all_files)\n",
    "    df= pd.DataFrame()   #load train and test separately\n",
    "    for filename in all_files:\n",
    "        temp_df = pd.read_csv(filename, usecols=['EC','Sequence'])\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# train_df = load_df('SwissprotDatasets/UnbalancedSwissprot/train')\n",
    "# test_df = load_df('SwissprotDatasets/test')\n",
    "# train_sequence = train_df['Sequence'].tolist() #[::10]\n",
    "# train_EC = train_df['EC'].tolist()#[::10]\n",
    "# test_sequence = test_df['Sequence'].tolist()\n",
    "# test_EC = test_df['EC'].tolist()\n",
    "# # test_sequence = test_sequence[::10]\n",
    "# len(train_sequence), len(train_EC), len(test_sequence), len(test_EC), len(train_df) #179587 5399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa0faf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5389131416528676, 0.1347282854132169, 0.21556525666114706)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cde5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_score(neighbor_ec, true_ec):\n",
    "    \"\"\"\n",
    "    This function calculates the overlap score between two EC numbers.\n",
    "    Args:\n",
    "        ec1 (str): The first EC number (e.g., \"1.2.3.4\").\n",
    "        ec2 (str): The second EC number (e.g., \"1.2.3.5\").\n",
    "    Returns:\n",
    "        int: The overlap score, which is the number of matching levels from the beginning.\n",
    "    \"\"\"\n",
    "    levels1 = neighbor_ec.split(\".\")\n",
    "    levels2 = true_ec.split(\".\")\n",
    "    alphaandbetabet = 0\n",
    "    alpha = len(levels1)\n",
    "    beta = len(levels2)\n",
    "    for i in range(min(len(levels1), len(levels2))):\n",
    "        if levels1[i] == levels2[i]:\n",
    "            alphaandbetabet += 1\n",
    "        else:\n",
    "            break\n",
    "    return alphaandbetabet, alpha, beta\n",
    "\n",
    "def hiclass_score_metrics(true_ecs, neighboring_ecs):\n",
    "    \"\"\"\n",
    "    This function calculates the hierarchical precision (hP) for a set of predicted EC numbers.\n",
    "    Args:\n",
    "        true_ecs (list): A list of true EC numbers (e.g., [\"1.2.3.4\", \"2.3.4.5\"]).\n",
    "        neighboring_ecs (list): A list of predicted EC numbers, where each element is a list of k nearest neighbors (e.g., [[\"1.2.3.5\", \"2.1.4.3\"], [\"3.4.5.6\", \"2.3.4.1\"]]).\n",
    "    Returns:\n",
    "        float: The average hierarchical precision across all predicted EC numbers. \n",
    "    \"\"\"\n",
    "    intersection = 0\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    for true_ec , neighbor_ecs in zip(true_ecs, neighboring_ecs):\n",
    "        max_score = 0\n",
    "        for neighbor_ec in neighbor_ecs:\n",
    "            alphaandbetabet, a, b = overlap_score(neighbor_ec, true_ec)\n",
    "            max_score = max(max_score, alphaandbetabet)\n",
    "        intersection += max_score\n",
    "        alpha += a\n",
    "        beta += b\n",
    "    hP = intersection/alpha\n",
    "    hR = intersection/beta\n",
    "    hF = 2*hP*hR/(hP+hR)\n",
    "    return  hP, hR, hF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c154265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_accuracy(accuracies, ec , retrieved_ec):\n",
    "#     print(ec, retrieved_ec)\n",
    "    for level, accuracy in accuracies.items():\n",
    "        for r_ec in retrieved_ec:\n",
    "            if ec.split('.')[:level] == r_ec.split('.')[:level]:\n",
    "                accuracies[level] += 1\n",
    "                break\n",
    "    return accuracies\n",
    "def top_k_retrieval(train_ec, test_ec, train_embeddings, test_embeddings, k):\n",
    "    print(f\"train_ec={len(train_ec)}, test_ec={len(test_ec)}, train_embeddings={len(train_embeddings)}, test_embeddings={len(test_embeddings)}\")\n",
    "    \"\"\"\n",
    "    Performs top-k NN retrieval using RoBERTa encoder embeddings and evaluates\n",
    "    EC number consistency in top-k retrievals.\n",
    "    Args:\n",
    "        model_name (str): Name of the RoBERTa model (e.g., 'roberta-base').\n",
    "        test_ec (list): List of known EC numbers of test batch.\n",
    "        train_seqs (list): List of sequences data for which to retrieve similar sequences.\n",
    "        test_seqs (list): List of sequences data against which retrivals are called.\n",
    "        k (int): Number of nearest neighbors to retrieve.\n",
    "    Returns:\n",
    "        tuple: (accuracy, retrieval_results)\n",
    "            - accuracy (float): Proportion of top-k retrievals with correct EC numbers.\n",
    "            - retrieval_results (list of lists): List of retrieved EC numbers for each text data point.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    index = faiss.IndexFlatL2(train_embeddings.size(1))  # Use L2 distance for cosine similarity\n",
    "    index.add(train_embeddings.cpu().detach().numpy())  # Add embeddings to the index\n",
    "    for k in K:\n",
    "        distances, retrieval_indices = index.search(test_embeddings.cpu().detach().numpy(), k)\n",
    "        retrieval_results = []\n",
    "        for i, retrieved_idxs in enumerate(retrieval_indices):\n",
    "    #         print(retrieved_idxs)\n",
    "            retrieved_ec_numbers = [train_ec[idx] for idx in retrieved_idxs]\n",
    "            retrieval_results.append(retrieved_ec_numbers)\n",
    "        accuracies = {} # initialize ec level accuracy\n",
    "        for level in range(4):\n",
    "            accuracies.setdefault(level+1, 0)\n",
    "        for i, retrieved_ec_numbers in enumerate(retrieval_results):\n",
    "            accuracies = level_accuracy(accuracies, test_ec[i], retrieved_ec_numbers)\n",
    "        for level, total_correct in accuracies.items():\n",
    "            accuracies[level] /= len(test_ec)\n",
    "        #calculate hiclass metrics\n",
    "        hiclass_metric = hiclass_score_metrics(test_ec, retrieval_results)\n",
    "        results[k] = accuracies, hiclass_metric\n",
    "    return results\n",
    "def get_embedding_stats(embeddings):\n",
    "    \"\"\"\n",
    "    This function extracts class token embedding, minimum, maximum and average \n",
    "    for each sequence from the encoder outputs.\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Encoder outputs of shape (batch_size, sequence_length, embedding_dim)\n",
    "    Returns:\n",
    "        tuple: A tuple containing three tensors:\n",
    "            - class_token_embeddings (torch.Tensor): Class token embeddings of shape (batch_size, embedding_dim)\n",
    "            - min_embeddings (torch.Tensor): Minimum embedding per sequence of shape (batch_size, embedding_dim)\n",
    "            - max_embeddings (torch.Tensor): Maximum embedding per sequence of shape (batch_size, embedding_dim)\n",
    "            - avg_embeddings (torch.Tensor): Average embedding per sequence of shape (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    class_token_embeddings = embeddings[:, 0, :]# Extract class token embeddings (assuming it's at index 0 for each sequence)\n",
    "    min_embeddings, _ = torch.min(embeddings, dim=1)\n",
    "    max_embeddings, _ = torch.max(embeddings, dim=1)\n",
    "    avg_embeddings = torch.mean(embeddings, dim=-2) # torch.sum(attention_mask*embeddings, axis=-2)/torch.sum(attention_mask, axis=1)\n",
    "    return class_token_embeddings, min_embeddings, max_embeddings, avg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a3ddb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/FinetunedModel/ were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at models/FinetunedModel/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BioTokenizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "# model = RobertaModel.from_pretrained(\"models/LOLBERTv4/\")\n",
    "# tokenizer = dl.load_tokenizer(\"LOLBERTv4/\") #FinetunedModel\n",
    "model = RobertaModel.from_pretrained(\"models/FinetunedModel/\")\n",
    "tokenizer = dl.load_tokenizer(\"FinetunedModel/\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\", trust_remote_code=True, local_files_only=True)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\", trust_remote_code=True, local_files_only=True)\n",
    "model.to(device)\n",
    "device_ids = [x for x in range(torch.cuda.device_count())]\n",
    "model = nn.DataParallel(model, device_ids=device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c1988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 #64 #dnabert\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
    "train_batch = tokenizer(train_sequence, padding='max_length', truncation=True) #lolbert\n",
    "test_batch = tokenizer(test_sequence, padding='max_length', truncation=True) #lolbert\n",
    "# train_batch = tokenizer(train_sequence, max_length=128, return_tensors='pt', truncation=True, padding=True) #dnabert\n",
    "# test_batch = tokenizer(test_sequence, max_length=128, return_tensors='pt', truncation=True, padding=True) #dnabert\n",
    "# train_batch = tokenizer.batch_encode_plus(train_sequence, max_length=128, return_tensors='pt', truncation=True, padding=True) #nuc\n",
    "# test_batch = tokenizer.batch_encode_plus(test_sequence, max_length=128, return_tensors='pt', truncation=True, padding=True) #nuc\n",
    "train_encodings = {'input_ids': torch.tensor(train_batch['input_ids']), 'attention_mask': torch.tensor(train_batch['attention_mask'])} #, 'labels': torch.tensor(batch['input_ids'])\n",
    "test_encodings = {'input_ids': torch.tensor(test_batch['input_ids']), 'attention_mask': torch.tensor(test_batch['attention_mask'])} #, 'labels': torch.tensor(batch['input_ids'])\n",
    "train_dataset = Dataset(train_encodings)\n",
    "test_dataset = Dataset(test_encodings)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66a1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296/296 [01:57<00:00,  2.52it/s]\n",
      "100%|██████████| 48/48 [00:18<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(loader):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    embeddings = []#torch.Tensor()#.to(device)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)#     labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask) # for lol and dna bert\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True) # for nuc trans\n",
    "        batch_embeddings = outputs.last_hidden_state.cpu() # LOLBERT\n",
    "#         batch_embeddings = outputs.hidden_states.cpu() #dnabert\n",
    "#         batch_embeddings = outputs['hidden_states'][-1].cpu() #for nuc trans\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings\n",
    "train_embeddings = get_embeddings(train_loader)\n",
    "test_embeddings = get_embeddings(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a771d",
   "metadata": {},
   "source": [
    "# Uniprot balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6412abc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's CLASS nearest retrieval stats {1: ({1: 0.39079097891812387, 2: 0.24530151985618565, 3: 0.22499591436509234, 4: 0.1785013891158686}, (0.25989745056381763, 0.25989745056381763, 0.25989745056381763)), 3: ({1: 0.572928583101814, 2: 0.3206406275535218, 3: 0.2739826769079915, 4: 0.20223892792940024}, (0.3424477038731819, 0.3424477038731819, 0.3424477038731819)), 5: ({1: 0.6685324399411668, 2: 0.37203791469194314, 3: 0.3066677561693087, 4: 0.2142506945579343}, (0.39037220134008827, 0.39037220134008827, 0.39037220134008827))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MIN nearest retrieval stats {1: ({1: 0.420248406602386, 2: 0.28297107370485375, 3: 0.25927439124039875, 4: 0.20603856839352835}, (0.29213310998529174, 0.29213310998529174, 0.29213310998529174)), 3: ({1: 0.6035299885602222, 2: 0.36345808138584734, 3: 0.31385847360679847, 4: 0.2390096420983821}, (0.37996404641281256, 0.37996404641281256, 0.3799640464128126)), 5: ({1: 0.6944353652557608, 2: 0.41330282725935613, 3: 0.3469929727079588, 4: 0.24987743095277007}, (0.4261521490439614, 0.4261521490439614, 0.4261521490439614))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MAX nearest retrieval stats {1: ({1: 0.4213106716783788, 2: 0.28280764830854715, 3: 0.25980552377839516, 4: 0.2078362477529008}, (0.2929400228795555, 0.2929400228795555, 0.2929400228795555)), 3: ({1: 0.605409380617748, 2: 0.3609658440921719, 3: 0.3112845236149698, 4: 0.23651740480470665}, (0.3785442882823991, 0.3785442882823991, 0.37854428828239906)), 5: ({1: 0.6948847850956038, 2: 0.4118728550416735, 3: 0.3447867298578199, 4: 0.248610884131394}, (0.4250388135316228, 0.4250388135316228, 0.4250388135316228))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's AVG nearest retrieval stats {1: ({1: 0.3793920575257395, 2: 0.23978591273083838, 3: 0.219602876286975, 4: 0.17539630658604347}, (0.2535442882823991, 0.2535442882823991, 0.2535442882823991)), 3: ({1: 0.5727243013564308, 2: 0.3187203791469194, 3: 0.2707550253309364, 4: 0.19831671841804216}, (0.3401291060630822, 0.3401291060630822, 0.3401291060630822)), 5: ({1: 0.6696355613662364, 2: 0.37530642261807484, 3: 0.31079424742605, 4: 0.2121261644059487}, (0.39196559895407745, 0.39196559895407745, 0.39196559895407745))}\n"
     ]
    }
   ],
   "source": [
    "#nuc trans\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14de4584",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s CLASS nearest retrieval stats {1: ({1: 0.38735904559568557, 2: 0.25747671188102633, 3: 0.23627226671024676, 4: 0.1992155580977284}, (0.27008089557117176, 0.27008089557117176, 0.27008089557117176)), 3: ({1: 0.5586288609249878, 2: 0.3242768426213434, 3: 0.27966170942964536, 4: 0.2222176826278804}, (0.3461962739009642, 0.3461962739009642, 0.3461962739009642)), 5: ({1: 0.6501879392057526, 2: 0.3665223075665959, 3: 0.3045432260173231, 4: 0.23042980879228633}, (0.3879208203954895, 0.3879208203954895, 0.3879208203954895))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MIN nearest retrieval stats {1: ({1: 0.4296453668900147, 2: 0.3099771204445171, 3: 0.28840496813204775, 4: 0.2448112436672659}, (0.31820967478346135, 0.31820967478346135, 0.31820967478346135)), 3: ({1: 0.5986272266710246, 2: 0.37877921228958983, 3: 0.337269161627717, 4: 0.2771286157868933}, (0.39795105409380616, 0.39795105409380616, 0.39795105409380616)), 5: ({1: 0.6904722993953261, 2: 0.43234188592907336, 3: 0.3740398757966988, 4: 0.29547311652230757}, (0.44808179441085144, 0.44808179441085144, 0.44808179441085144))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MAX nearest retrieval stats {1: ({1: 0.43152475894754044, 2: 0.31369504821049193, 3: 0.289181238764504, 4: 0.24199215558097728}, (0.3190983003758784, 0.3190983003758784, 0.3190983003758784)), 3: ({1: 0.6020591599934629, 2: 0.38866644876613826, 3: 0.3417225036770714, 4: 0.2785585880045759}, (0.4027516751103121, 0.4027516751103121, 0.4027516751103121)), 5: ({1: 0.6888380454322601, 2: 0.4394917470174865, 3: 0.37485700277823175, 4: 0.2942474260500082}, (0.44935855531949664, 0.44935855531949664, 0.44935855531949664))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s AVG nearest retrieval stats {1: ({1: 0.368932832162118, 2: 0.23108351037751265, 3: 0.2088168001307403, 4: 0.1749060303971237}, (0.24593479326687367, 0.24593479326687367, 0.24593479326687367)), 3: ({1: 0.5491093315901291, 2: 0.30086615460042493, 3: 0.25138911586860596, 4: 0.19320967478346135}, (0.32364356921065535, 0.32364356921065535, 0.32364356921065535)), 5: ({1: 0.6477774146102304, 2: 0.3512828893610067, 3: 0.28129596339271123, 4: 0.20420003268507927}, (0.3711390750122569, 0.3711390750122569, 0.3711390750122569))}\n"
     ]
    }
   ],
   "source": [
    "#lolbert\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322c8458",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/FinetunedModel/'s CLASS nearest retrieval stats {1: ({1: 0.5789753227651577, 2: 0.4745464945252492, 3: 0.4577545350547475, 4: 0.3998610884131394}, (0.4777843601895735, 0.4777843601895735, 0.4777843601895735)), 3: ({1: 0.7128207223402516, 2: 0.5489050498447459, 3: 0.5231246935773819, 4: 0.4584082366399739}, (0.5608146756005883, 0.5608146756005883, 0.5608146756005883)), 5: ({1: 0.7759029253145939, 2: 0.590047393364929, 3: 0.5528681157051806, 4: 0.4815737865664324}, (0.6000980552377839, 0.6000980552377839, 0.6000980552377839))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MIN nearest retrieval stats {1: ({1: 0.5892302663833959, 2: 0.49538323255433897, 3: 0.47589475404477855, 4: 0.41661219153456447}, (0.4942801111292695, 0.4942801111292695, 0.4942801111292695)), 3: ({1: 0.7270795881680013, 2: 0.57431769897042, 3: 0.5412649125674129, 4: 0.4810835103775127}, (0.5809364275208367, 0.5809364275208367, 0.5809364275208367)), 5: ({1: 0.792082039548946, 2: 0.6183199869259683, 3: 0.5736639973851937, 4: 0.5070681483902598}, (0.622783543062592, 0.622783543062592, 0.622783543062592))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MAX nearest retrieval stats {1: ({1: 0.5975649615950319, 2: 0.4977120444517078, 3: 0.4804298087922863, 4: 0.4196355613662363}, (0.49883559405131556, 0.49883559405131556, 0.49883559405131556)), 3: ({1: 0.7276515770550743, 2: 0.5706406275535219, 3: 0.5422863212943292, 4: 0.47940840006537017}, (0.5799967314920739, 0.5799967314920739, 0.5799967314920739)), 5: ({1: 0.7926131720869423, 2: 0.6111292694884785, 3: 0.5722748815165877, 4: 0.5038404968132048}, (0.6199644549763034, 0.6199644549763034, 0.6199644549763034))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For models/FinetunedModel/'s AVG nearest retrieval stats {1: ({1: 0.5048619055401209, 2: 0.39271122732472624, 3: 0.3719970583428665, 4: 0.3177398267690799}, (0.3968275044941984, 0.3968275044941984, 0.3968275044941984)), 3: ({1: 0.6611374407582938, 2: 0.47327994770387316, 3: 0.4354469684588985, 4: 0.3719153456447132}, (0.4854449256414447, 0.4854449256414447, 0.4854449256414447)), 5: ({1: 0.741215884948521, 2: 0.52022389279294, 3: 0.4712779865991175, 4: 0.39585716620362804}, (0.5321437326360516, 0.5321437326360516, 0.5321437326360516))}\n"
     ]
    }
   ],
   "source": [
    "#finetuned\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e5aa4dd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's CLASS nearest retrieval stats {1: ({1: 0.3354714822683445, 2: 0.20820395489459062, 3: 0.18773492400719072, 4: 0.15476385030233697}, (0.2215435528681157, 0.2215435528681157, 0.2215435528681157)), 3: ({1: 0.5089883967968623, 2: 0.26295146265729696, 3: 0.21690635724791632, 4: 0.16632619709102794}, (0.28879310344827586, 0.28879310344827586, 0.28879310344827586)), 5: ({1: 0.60818761235496, 2: 0.30756659584899493, 3: 0.24133845399575093, 4: 0.1711881026311489}, (0.3320701912077137, 0.3320701912077137, 0.3320701912077137))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MIN nearest retrieval stats {1: ({1: 0.3346543552868116, 2: 0.2076728223565942, 3: 0.18552868115705182, 4: 0.15643896061447948}, (0.22107370485373426, 0.22107370485373426, 0.22107370485373426)), 3: ({1: 0.5081304134662527, 2: 0.26307403170452687, 3: 0.21576237947377022, 4: 0.16648962248733454}, (0.28836411178297106, 0.28836411178297106, 0.28836411178297106)), 5: ({1: 0.6059813695048211, 2: 0.3039303807811734, 3: 0.23713025004085636, 4: 0.17028926295146266}, (0.32933281581957835, 0.32933281581957835, 0.32933281581957835))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MAX nearest retrieval stats {1: ({1: 0.32803562673639486, 2: 0.210124203301193, 3: 0.1884294819414937, 4: 0.15983003758784115}, (0.22160483739173067, 0.22160483739173067, 0.22160483739173067)), 3: ({1: 0.5048619055401209, 2: 0.26270632456283705, 3: 0.2177643405785259, 4: 0.16951299231900638}, (0.2887113907501226, 0.2887113907501226, 0.2887113907501226)), 5: ({1: 0.6122732472626247, 2: 0.30630004902761887, 3: 0.23982676907991501, 4: 0.17429318516097403}, (0.33317331263278316, 0.33317331263278316, 0.33317331263278316))}\n",
      "train_ec=234200, test_ec=24476, train_embeddings=234200, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's AVG nearest retrieval stats {1: ({1: 0.3425396306586043, 2: 0.20832652394182055, 3: 0.1849158359209021, 4: 0.15541755188756332}, (0.22279988560222258, 0.22279988560222258, 0.22279988560222258)), 3: ({1: 0.5178950808955711, 2: 0.26785422454649455, 3: 0.21862232390913547, 4: 0.16820558914855369}, (0.29314430462493873, 0.29314430462493873, 0.29314430462493873)), 5: ({1: 0.6182791305768917, 2: 0.31402189900310506, 3: 0.24423925478019284, 4: 0.17478346134989378}, (0.3378309364275208, 0.3378309364275208, 0.3378309364275208))}\n"
     ]
    }
   ],
   "source": [
    "#dnabert\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1134c",
   "metadata": {},
   "source": [
    "# Uniprot unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c998c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0496fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254a0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=278772, test_ec=24476, train_embeddings=278772, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's CLASS nearest retrieval stats {1: ({1: 0.5469439450890669, 2: 0.44030887399901947, 3: 0.42082039548945904, 4: 0.35974015361987255}, (0.4419533420493545, 0.4419533420493545, 0.4419533420493545)), 3: ({1: 0.7342294492564144, 2: 0.5431443046249387, 3: 0.5033910769733616, 4: 0.4115460042490603}, (0.5480777087759438, 0.5480777087759438, 0.5480777087759438)), 5: ({1: 0.8202320640627554, 2: 0.602631148880536, 3: 0.550049027618892, 4: 0.43802091845072727}, (0.6027332897532276, 0.6027332897532276, 0.6027332897532276))}\n",
      "train_ec=278772, test_ec=24476, train_embeddings=278772, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MIN nearest retrieval stats {1: ({1: 0.6007517568230103, 2: 0.5055156071253473, 3: 0.4866808302010132, 4: 0.41608105899656805}, (0.5022573132864847, 0.5022573132864847, 0.5022573132864847)), 3: ({1: 0.7632374571008335, 2: 0.6000163425396307, 3: 0.5616113744075829, 4: 0.4653129596339271}, (0.5975445334204935, 0.5975445334204935, 0.5975445334204935)), 5: ({1: 0.8341232227488151, 2: 0.6501470828566759, 3: 0.6019365909462331, 4: 0.4876205262297761}, (0.643456855695375, 0.643456855695375, 0.643456855695375))}\n",
      "train_ec=278772, test_ec=24476, train_embeddings=278772, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MAX nearest retrieval stats {1: ({1: 0.6011603203137768, 2: 0.5030233698316718, 3: 0.4845563000490276, 4: 0.41440594868442554}, (0.5007864847197254, 0.5007864847197254, 0.5007864847197254)), 3: ({1: 0.7625020428174538, 2: 0.598014381434875, 3: 0.5620199378983494, 4: 0.4631884294819415}, (0.5964311979081549, 0.5964311979081549, 0.5964311979081549)), 5: ({1: 0.8380454322601733, 2: 0.6550907010949502, 3: 0.6075339107697336, 4: 0.4882333714659258}, (0.6472258538976957, 0.6472258538976957, 0.6472258538976957))}\n",
      "train_ec=278772, test_ec=24476, train_embeddings=278772, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's AVG nearest retrieval stats {1: ({1: 0.5282725935610394, 2: 0.41334368360843277, 3: 0.39307893446641606, 4: 0.33048700768099365}, (0.4162955548292205, 0.4162955548292205, 0.4162955548292204)), 3: ({1: 0.7231573786566432, 2: 0.5195293348586371, 3: 0.476303317535545, 4: 0.3841313940186305}, (0.525780356267364, 0.525780356267364, 0.525780356267364)), 5: ({1: 0.8117339434548129, 2: 0.5856757640137278, 3: 0.5288445824481124, 4: 0.41440594868442554}, (0.5851650596502697, 0.5851650596502697, 0.5851650596502697))}\n"
     ]
    }
   ],
   "source": [
    "#nuc trans\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0df69e48",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's CLASS nearest retrieval stats {1: ({1: 0.5065370158522634, 2: 0.41130086615460043, 3: 0.39311979081549275, 4: 0.3463392711227325}, (0.41432423598627227, 0.41432423598627227, 0.41432423598627227)), 3: ({1: 0.6646919431279621, 2: 0.48561856512502044, 3: 0.44243340415100507, 4: 0.3767772511848341}, (0.4923802908972054, 0.4923802908972054, 0.4923802908972054)), 5: ({1: 0.7464863539794084, 2: 0.5341150514789998, 3: 0.4747916326197091, 4: 0.3921800947867299}, (0.5368932832162118, 0.5368932832162118, 0.5368932832162118))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MIN nearest retrieval stats {1: ({1: 0.5123794737702239, 2: 0.4178378820068639, 3: 0.39945252492237293, 4: 0.3514054584082366}, (0.42026883477692434, 0.42026883477692434, 0.4202688347769243)), 3: ({1: 0.6725772184997548, 2: 0.491910442882824, 3: 0.4499101160320314, 4: 0.3834776924334041}, (0.4994688674620036, 0.4994688674620036, 0.4994688674620036)), 5: ({1: 0.7551478999836575, 2: 0.5399575093969603, 3: 0.4812060794247426, 4: 0.39965680666775616}, (0.5439920738682791, 0.5439920738682791, 0.5439920738682791))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MAX nearest retrieval stats {1: ({1: 0.5135643078934466, 2: 0.4215558097728387, 3: 0.4044778558588005, 4: 0.3565533583918941}, (0.42403783297924497, 0.42403783297924497, 0.42403783297924497)), 3: ({1: 0.6725772184997548, 2: 0.4942392547801928, 3: 0.453995750939696, 4: 0.3897695701912077}, (0.5026454486027129, 0.5026454486027129, 0.5026454486027129)), 5: ({1: 0.7545759110965844, 2: 0.5404886419349567, 3: 0.4832080405294983, 4: 0.40525412649125675}, (0.545881680013074, 0.545881680013074, 0.545881680013074))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's AVG nearest retrieval stats {1: ({1: 0.5078444190227162, 2: 0.4103203137767609, 3: 0.3911586860598137, 4: 0.3435610393855205}, (0.4132211145612028, 0.4132211145612028, 0.4132211145612028)), 3: ({1: 0.6752328811897369, 2: 0.4950972381108024, 3: 0.4523614969766302, 4: 0.3847850956038568}, (0.5018691779702565, 0.5018691779702565, 0.5018691779702565)), 5: ({1: 0.7560467396633437, 2: 0.549231900637359, 3: 0.4911341722503677, 4: 0.40537669553848665}, (0.5504473770223893, 0.5504473770223893, 0.5504473770223893))}\n"
     ]
    }
   ],
   "source": [
    "#dnabert\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf897f8e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/FinetunedModel/'s CLASS nearest retrieval stats {1: ({1: 0.7243013564307893, 2: 0.6640382415427357, 3: 0.6531704526883477, 4: 0.5842457917960451}, (0.6564389606144795, 0.6564389606144795, 0.6564389606144795)), 3: ({1: 0.8104265402843602, 2: 0.7185814675600588, 3: 0.6995015525412649, 4: 0.6228958980225527}, (0.7128513646020591, 0.7128513646020591, 0.7128513646020591)), 5: ({1: 0.849280928256251, 2: 0.7451380944598791, 3: 0.7200114397777415, 4: 0.638462167020755}, (0.7382231573786566, 0.7382231573786566, 0.7382231573786566))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MIN nearest retrieval stats {1: ({1: 0.762828893610067, 2: 0.7116767445661055, 3: 0.7026883477692434, 4: 0.633845399575094}, (0.7027598463801275, 0.7027598463801275, 0.7027598463801275)), 3: ({1: 0.8366154600424907, 2: 0.7549027618891976, 3: 0.7394999182873019, 4: 0.6657133518548782}, (0.7491828730184671, 0.7491828730184671, 0.7491828730184671)), 5: ({1: 0.871016505965027, 2: 0.7788854387971891, 3: 0.7585798333060958, 4: 0.6786648145121752}, (0.7717866481451218, 0.7717866481451218, 0.7717866481451218))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MAX nearest retrieval stats {1: ({1: 0.7623794737702239, 2: 0.7131884294819415, 3: 0.7028926295146266, 4: 0.632578852753718}, (0.7027598463801275, 0.7027598463801275, 0.7027598463801275)), 3: ({1: 0.836329465598954, 2: 0.7564144468050334, 3: 0.7393364928909952, 4: 0.6646510867788854}, (0.7491828730184671, 0.7491828730184671, 0.7491828730184671)), 5: ({1: 0.8699950972381107, 2: 0.7801519856185651, 3: 0.7582938388625592, 4: 0.6786648145121752}, (0.7717764340578526, 0.7717764340578526, 0.7717764340578525))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/FinetunedModel/'s AVG nearest retrieval stats {1: ({1: 0.6999918287301846, 2: 0.6354796535381598, 3: 0.6232227488151659, 4: 0.5574848831508417}, (0.629044778558588, 0.629044778558588, 0.629044778558588)), 3: ({1: 0.7983330609576729, 2: 0.6952524922372937, 3: 0.6728223565942147, 4: 0.5990766465108678}, (0.6913711390750122, 0.6913711390750122, 0.6913711390750122)), 5: ({1: 0.8441738846216702, 2: 0.7267527373753881, 3: 0.6981124366726589, 4: 0.616277169472136}, (0.7213290570354634, 0.7213290570354634, 0.7213290570354634))}\n"
     ]
    }
   ],
   "source": [
    "#finetune\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3610047e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s CLASS nearest retrieval stats {1: ({1: 0.5946641608105899, 2: 0.510745219807158, 3: 0.49415754208203955, 4: 0.441861415263932}, (0.5103570844909299, 0.5103570844909299, 0.5103570844909299)), 3: ({1: 0.7292449746690636, 2: 0.5856349076646511, 3: 0.5518058506291877, 4: 0.4814103611701258}, (0.5870240235332571, 0.5870240235332571, 0.5870240235332571)), 5: ({1: 0.7922863212943292, 2: 0.62828893610067, 3: 0.5830609576728224, 4: 0.49991828730184673}, (0.6258886255924171, 0.6258886255924171, 0.6258886255924171))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MIN nearest retrieval stats {1: ({1: 0.6759274391240399, 2: 0.6088004575911097, 3: 0.5949910116032031, 4: 0.5349730348096094}, (0.6036729857819905, 0.6036729857819905, 0.6036729857819905)), 3: ({1: 0.7821539467233208, 2: 0.6713106716783788, 3: 0.6456937408073214, 4: 0.5744402680176499}, (0.6683996568066678, 0.6683996568066678, 0.6683996568066678)), 5: ({1: 0.8315492727569864, 2: 0.7064879882333714, 3: 0.6735577708775944, 4: 0.5950318679522798}, (0.701656724955058, 0.701656724955058, 0.701656724955058))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MAX nearest retrieval stats {1: ({1: 0.677765974832489, 2: 0.6114561202810916, 3: 0.5970338290570355, 4: 0.5361987252819088}, (0.6056136623631312, 0.6056136623631312, 0.6056136623631312)), 3: ({1: 0.7848096094133028, 2: 0.6731900637359045, 3: 0.6470420003268508, 4: 0.5758702402353326}, (0.6702279784278476, 0.6702279784278476, 0.6702279784278476)), 5: ({1: 0.8352263441738846, 2: 0.7093070763196601, 3: 0.6743748978591273, 4: 0.5943781663670534}, (0.7033216211799314, 0.7033216211799314, 0.7033216211799314))}\n",
      "train_ec=557543, test_ec=24476, train_embeddings=557543, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s AVG nearest retrieval stats {1: ({1: 0.5604673966334368, 2: 0.46588494852100015, 3: 0.4484392874652721, 4: 0.3947949011276352}, (0.4673966334368361, 0.4673966334368361, 0.4673966334368361)), 3: ({1: 0.7086125183853571, 2: 0.5469030887399902, 3: 0.5111129269488478, 4: 0.4363866644876614}, (0.5507537996404641, 0.5507537996404641, 0.5507537996404641)), 5: ({1: 0.7764340578525903, 2: 0.595481287792123, 3: 0.5471073704853734, 4: 0.4588985128288936}, (0.5944803072397451, 0.5944803072397451, 0.5944803072397451))}\n"
     ]
    }
   ],
   "source": [
    "#lolbert\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_EC, test_EC, train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6c609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef290f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3ea42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7b516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30848c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4915efe5",
   "metadata": {},
   "source": [
    "# swissprot Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c4177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b47d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's CLASS nearest retrieval stats {1: ({1: 0.3315492727569864, 2: 0.12857493054420657, 3: 0.10732962902435038, 4: 0.0539303807811734}, (0.1553460532766792, 0.1553460532766792, 0.1553460532766792)), 3: ({1: 0.5115214904396144, 2: 0.20665141362967807, 3: 0.16126000980552377, 4: 0.07292858310181402}, (0.23809037424415755, 0.23809037424415755, 0.23809037424415755)), 5: ({1: 0.5943373100179767, 2: 0.2555973198235006, 3: 0.19382252001961106, 4: 0.08281581957836248}, (0.2816432423598627, 0.2816432423598627, 0.2816432423598627))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MIN nearest retrieval stats {1: ({1: 0.36590946233044613, 2: 0.153374734433731, 3: 0.12497957182546168, 4: 0.06635071090047394}, (0.1776536198725282, 0.1776536198725282, 0.17765361987252823)), 3: ({1: 0.533175355450237, 2: 0.23022552704690308, 3: 0.17707141689818598, 4: 0.08604347115541755}, (0.2566289426376859, 0.2566289426376859, 0.2566289426376859)), 5: ({1: 0.6186059813695048, 2: 0.2783951626082693, 3: 0.20918450727243013, 4: 0.09654355286811571}, (0.30068230102958, 0.30068230102958, 0.30068230102958))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's MAX nearest retrieval stats {1: ({1: 0.3638666448766138, 2: 0.15137277332897534, 3: 0.12718581467560058, 4: 0.06802582121261644}, (0.17761276352345154, 0.17761276352345154, 0.17761276352345157)), 3: ({1: 0.5343193332243831, 2: 0.2312060794247426, 3: 0.18299558751429973, 4: 0.09147736558261153}, (0.25999959143650925, 0.25999959143650925, 0.25999959143650925)), 5: ({1: 0.6166040202647491, 2: 0.2797434221277987, 3: 0.2145775453505475, 4: 0.1012828893610067}, (0.3030519692760255, 0.3030519692760255, 0.3030519692760255))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's AVG nearest retrieval stats {1: ({1: 0.3412322274881517, 2: 0.1251838535708449, 3: 0.0983003758784115, 4: 0.04612681810753391}, (0.1527108187612355, 0.1527108187612355, 0.1527108187612355)), 3: ({1: 0.5179359372446478, 2: 0.20824481124366725, 3: 0.1530070272920412, 4: 0.06144794901127635}, (0.23515893119790815, 0.23515893119790815, 0.23515893119790815)), 5: ({1: 0.599403497303481, 2: 0.25764013727733287, 3: 0.1876123549599608, 4: 0.07121261644059487}, (0.2789671514953424, 0.2789671514953424, 0.2789671514953424))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1573b556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's CLASS nearest retrieval stats {1: ({1: 0.253350220624285, 2: 0.09862722667102468, 3: 0.07680993626409544, 4: 0.038486680830201014}, (0.11681851609740154, 0.11681851609740154, 0.11681851609740154)), 3: ({1: 0.42625428991665304, 2: 0.16497793757149862, 3: 0.11615460042490602, 4: 0.05417551887563327}, (0.19039058669717274, 0.19039058669717274, 0.19039058669717274)), 5: ({1: 0.5126246118646838, 2: 0.21008334695211636, 3: 0.14340578525902925, 4: 0.06259192678542246}, (0.23217641771531297, 0.23217641771531297, 0.23217641771531297))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MIN nearest retrieval stats {1: ({1: 0.24052132701421802, 2: 0.08714659258048701, 3: 0.06577872201340089, 4: 0.0298659911750286}, (0.10582815819578363, 0.10582815819578363, 0.10582815819578363)), 3: ({1: 0.4150596502696519, 2: 0.15774636378493218, 3: 0.1072479163261971, 4: 0.04265402843601896}, (0.18067698970420004, 0.18067698970420004, 0.18067698970420004)), 5: ({1: 0.5015525412649126, 2: 0.19749959143650922, 3: 0.1315983003758784, 4: 0.049477038731818926}, (0.22003186795227977, 0.22003186795227977, 0.22003186795227977))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MAX nearest retrieval stats {1: ({1: 0.24125674129759764, 2: 0.0937244647818271, 3: 0.07006863866644876, 4: 0.03476875306422618}, (0.10995464945252492, 0.10995464945252492, 0.10995464945252492)), 3: ({1: 0.4187367216865501, 2: 0.1604837391730675, 3: 0.11317208694231083, 4: 0.048537342703056055}, (0.18523247262624612, 0.18523247262624612, 0.18523247262624615)), 5: ({1: 0.5093969602876287, 2: 0.20272920411832, 3: 0.14124039875796698, 4: 0.056994606961921886}, (0.22759029253145938, 0.22759029253145938, 0.22759029253145938))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's AVG nearest retrieval stats {1: ({1: 0.25649615950318677, 2: 0.11166040202647491, 3: 0.08767772511848342, 4: 0.045023696682464455}, (0.1252144958326524, 0.1252144958326524, 0.1252144958326524)), 3: ({1: 0.42380290897205425, 2: 0.1827095930707632, 3: 0.1377267527373754, 4: 0.06655499264585717}, (0.2026985618565125, 0.2026985618565125, 0.2026985618565125)), 5: ({1: 0.5112763523451545, 2: 0.23038895244320967, 3: 0.1697989867625429, 4: 0.07991501879392057}, (0.2478448275862069, 0.2478448275862069, 0.2478448275862069))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887182a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/FinetunedModel/'s CLASS nearest retrieval stats {1: ({1: 0.44251511684915834, 2: 0.32537996404641284, 3: 0.305360352998856, 4: 0.25874325870240233}, (0.33299967314920736, 0.33299967314920736, 0.33299967314920736)), 3: ({1: 0.5817944108514463, 2: 0.4074603693413957, 3: 0.37600098055237785, 4: 0.3129187775780356}, (0.41954363458081384, 0.41954363458081384, 0.41954363458081384)), 5: ({1: 0.6546821376041837, 2: 0.4546085961758457, 3: 0.4122405621833633, 4: 0.3328158195783625}, (0.4635867788854388, 0.4635867788854388, 0.4635867788854388))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MIN nearest retrieval stats {1: ({1: 0.4755270469030887, 2: 0.35263114888053604, 3: 0.331712698153293, 4: 0.2817045268834777}, (0.36039385520509887, 0.36039385520509887, 0.36039385520509887)), 3: ({1: 0.6046331099852917, 2: 0.43070763196600753, 3: 0.3976548455630005, 4: 0.3294247426050008}, (0.4406050825298251, 0.4406050825298251, 0.4406050825298251)), 5: ({1: 0.6659584899493382, 2: 0.46968458898512827, 3: 0.4269897042000327, 4: 0.3494443536525576}, (0.4780192841967642, 0.4780192841967642, 0.4780192841967642))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MAX nearest retrieval stats {1: ({1: 0.4654763850302337, 2: 0.3523860107860762, 3: 0.3319578362477529, 4: 0.28105082529825137}, (0.3577177643405785, 0.3577177643405785, 0.3577177643405785)), 3: ({1: 0.5982595195293349, 2: 0.4276842621343357, 3: 0.39667429318516095, 4: 0.3300375878411505}, (0.4381639156724955, 0.4381639156724955, 0.4381639156724955)), 5: ({1: 0.6584000653701585, 2: 0.4659258048700768, 3: 0.42543716293512013, 4: 0.34825951952933487}, (0.4745056381761726, 0.4745056381761726, 0.4745056381761726))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/FinetunedModel/'s AVG nearest retrieval stats {1: ({1: 0.3780437980062102, 2: 0.24387154763850302, 3: 0.22029743422127798, 4: 0.17396633436836084}, (0.254044778558588, 0.254044778558588, 0.254044778558588)), 3: ({1: 0.5299885602222585, 2: 0.3279130576891649, 3: 0.28493217846053276, 4: 0.21862232390913547}, (0.34036403007027294, 0.34036403007027294, 0.34036403007027294)), 5: ({1: 0.6015688838045432, 2: 0.37105736231410363, 3: 0.3175355450236967, 4: 0.23757966988069945}, (0.3819353652557608, 0.3819353652557608, 0.3819353652557608))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42138074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s CLASS nearest retrieval stats {1: ({1: 0.2643405785259029, 2: 0.11468377185814675, 3: 0.09286648145121752, 4: 0.061121098218663183}, (0.13325298251348258, 0.13325298251348258, 0.13325298251348258)), 3: ({1: 0.42870567086125183, 2: 0.17678542245464945, 3: 0.12980062101650597, 4: 0.07991501879392057}, (0.20380168328158196, 0.20380168328158196, 0.20380168328158196)), 5: ({1: 0.5132783134499102, 2: 0.2142506945579343, 3: 0.15137277332897534, 4: 0.08759601242033012}, (0.24162444843928746, 0.24162444843928746, 0.24162444843928746))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MIN nearest retrieval stats {1: ({1: 0.29596339271122735, 2: 0.15562183363294657, 3: 0.13453995750939696, 4: 0.09879065206733126}, (0.17122895898022553, 0.17122895898022553, 0.17122895898022553)), 3: ({1: 0.4697663016832816, 2: 0.22712044451707794, 3: 0.17976793593724466, 4: 0.12420330119300539}, (0.2502144958326524, 0.2502144958326524, 0.2502144958326524)), 5: ({1: 0.553276679195947, 2: 0.2723484229449256, 3: 0.21041019774472952, 4: 0.13776760908645203}, (0.2934507272430136, 0.2934507272430136, 0.2934507272430136))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MAX nearest retrieval stats {1: ({1: 0.3050743585553195, 2: 0.15701094950155253, 3: 0.13441738846216703, 4: 0.10030233698316718}, (0.17420125837555156, 0.17420125837555156, 0.17420125837555156)), 3: ({1: 0.47115541755188756, 2: 0.22867298578199052, 3: 0.1836084327504494, 4: 0.12837064879882334}, (0.2529518712207877, 0.2529518712207877, 0.2529518712207877)), 5: ({1: 0.5518875633273411, 2: 0.27128615786893284, 3: 0.21200359535871874, 4: 0.1417715312959634}, (0.294237211962739, 0.294237211962739, 0.294237211962739))}\n",
      "train_ec=22280, test_ec=24476, train_embeddings=22280, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s AVG nearest retrieval stats {1: ({1: 0.25567903252165386, 2: 0.09597156398104266, 3: 0.07055891485536853, 4: 0.03938552050988724}, (0.11539875796698806, 0.11539875796698806, 0.11539875796698806)), 3: ({1: 0.42956365419186143, 2: 0.16771531295963393, 3: 0.11117012583755516, 4: 0.055564634744239255}, (0.19100343193332245, 0.19100343193332245, 0.19100343193332245)), 5: ({1: 0.5143814348749796, 2: 0.21045105409380618, 3: 0.13690962575584245, 4: 0.06422618074848832}, (0.23149207386827914, 0.23149207386827914, 0.23149207386827914))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d1f9d",
   "metadata": {},
   "source": [
    "# swissprot UnBalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9459ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s CLASS nearest retrieval stats {1: ({1: 0.38780846543552866, 2: 0.2613172086942311, 3: 0.24146102304298087, 4: 0.2079996731492074}, (0.274646592580487, 0.274646592580487, 0.274646592580487)), 3: ({1: 0.5561366236313123, 2: 0.35169145285177317, 3: 0.31161137440758296, 4: 0.25988723647654843}, (0.3698316718418042, 0.3698316718418042, 0.3698316718418042)), 5: ({1: 0.6452443209674783, 2: 0.4076237947377022, 3: 0.3516097401536199, 4: 0.28448275862068967}, (0.42224015361987255, 0.42224015361987255, 0.42224015361987255))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MIN nearest retrieval stats {1: ({1: 0.4686223239091355, 2: 0.3577790488641935, 3: 0.341885929073378, 4: 0.30646347442392546}, (0.3686876940676581, 0.3686876940676581, 0.3686876940676581)), 3: ({1: 0.6221196273900964, 2: 0.4451299231900637, 3: 0.4129351201176663, 4: 0.36366236313123057}, (0.4609617584572643, 0.4609617584572643, 0.4609617584572643)), 5: ({1: 0.6956201993789835, 2: 0.49235986272266713, 3: 0.44913384539957507, 4: 0.3883804543226017}, (0.5063735904559569, 0.5063735904559569, 0.5063735904559569))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s MAX nearest retrieval stats {1: ({1: 0.46613008661546, 2: 0.35818761235495994, 3: 0.3386174211472463, 4: 0.3049926458571662}, (0.3669819414937081, 0.3669819414937081, 0.36698194149370816)), 3: ({1: 0.621016505965027, 2: 0.4499918287301847, 3: 0.4140382415427357, 4: 0.3644386337636869}, (0.4623713025004086, 0.4623713025004086, 0.4623713025004086)), 5: ({1: 0.6985210001634254, 2: 0.5000408563490767, 3: 0.4521980715803236, 4: 0.389851282889361}, (0.5101528027455466, 0.5101528027455466, 0.5101528027455466))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/LOLBERTv4/'s AVG nearest retrieval stats {1: ({1: 0.35254943618238277, 2: 0.21277986599117502, 3: 0.1902680176499428, 4: 0.1545187122078771}, (0.22752900800784442, 0.22752900800784442, 0.22752900800784442)), 3: ({1: 0.5400800784441903, 2: 0.31255107043634583, 3: 0.26568883804543225, 4: 0.20873508743258704}, (0.33176376858963885, 0.33176376858963885, 0.33176376858963885)), 5: ({1: 0.6406684098708939, 2: 0.3705262297761072, 3: 0.30952770060467394, 4: 0.23753881353162282}, (0.3895652884458245, 0.3895652884458245, 0.3895652884458245))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942d8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/FinetunedModel/'s CLASS nearest retrieval stats {1: ({1: 0.5589965680666775, 2: 0.47184997548619056, 3: 0.4575502533093643, 4: 0.41232227488151657}, (0.47517976793593725, 0.47517976793593725, 0.47517976793593725)), 3: ({1: 0.6759682954731165, 2: 0.5498447458735087, 3: 0.5245138094459879, 4: 0.46935773819251514}, (0.554921147246282, 0.554921147246282, 0.554921147246282)), 5: ({1: 0.7285095603856839, 2: 0.5863703219480307, 3: 0.5526229776107207, 4: 0.48986762542899165}, (0.5893426213433568, 0.5893426213433568, 0.5893426213433568))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MIN nearest retrieval stats {1: ({1: 0.6061856512502043, 2: 0.5288445824481124, 3: 0.5158114070926622, 4: 0.4702157215231247}, (0.5302643405785259, 0.5302643405785259, 0.5302643405785259)), 3: ({1: 0.7120444517077954, 2: 0.5992400719071743, 3: 0.5783216211799314, 4: 0.5250857983330609}, (0.6036729857819905, 0.6036729857819905, 0.6036729857819905)), 5: ({1: 0.7591926785422455, 2: 0.6320885765647982, 3: 0.6050416734760582, 4: 0.5432668736721686}, (0.6348974505638176, 0.6348974505638176, 0.6348974505638176))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/FinetunedModel/'s MAX nearest retrieval stats {1: ({1: 0.6053685242686714, 2: 0.5269243340415101, 3: 0.5138503023369831, 4: 0.46821376041836904}, (0.5285892302663834, 0.5285892302663834, 0.5285892302663834)), 3: ({1: 0.7143732636051643, 2: 0.5979735250857984, 3: 0.5760745219807158, 4: 0.52010132374571}, (0.6021306586043471, 0.6021306586043471, 0.6021306586043471)), 5: ({1: 0.7650759928092825, 2: 0.6347850956038569, 3: 0.6071253472789672, 4: 0.5408154927275699}, (0.6369504821049191, 0.6369504821049191, 0.6369504821049191))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For models/FinetunedModel/'s AVG nearest retrieval stats {1: ({1: 0.5220215721523125, 2: 0.4262951462657297, 3: 0.4098708939369178, 4: 0.36791142343520183}, (0.43152475894754044, 0.43152475894754044, 0.4315247589475405)), 3: ({1: 0.656275535218173, 2: 0.5142997221768263, 3: 0.4841885929073378, 4: 0.42568230102958}, (0.5201115378329793, 0.5201115378329793, 0.5201115378329793)), 5: ({1: 0.7190717437489786, 2: 0.5571171760091518, 3: 0.5187530642261807, 4: 0.45227978427847687}, (0.561805442065697, 0.561805442065697, 0.561805442065697))}\n"
     ]
    }
   ],
   "source": [
    "#finetuned\n",
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e63901",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's CLASS nearest retrieval stats {1: ({1: 0.33686059813695046, 2: 0.19570191207713677, 3: 0.17617257721849974, 4: 0.1305360352998856}, (0.20981778068311815, 0.20981778068311815, 0.20981778068311815)), 3: ({1: 0.5189164896224874, 2: 0.29069292368033994, 3: 0.2474669063572479, 4: 0.17780683118156562}, (0.30872078771041017, 0.30872078771041017, 0.30872078771041017)), 5: ({1: 0.6122732472626247, 2: 0.3491175028599444, 3: 0.28869096257558424, 4: 0.201340088249714}, (0.3628554502369668, 0.3628554502369668, 0.3628554502369668))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MIN nearest retrieval stats {1: ({1: 0.3057689164896225, 2: 0.17535545023696683, 3: 0.15096420983820885, 4: 0.10630822029743422}, (0.1845991992155581, 0.1845991992155581, 0.1845991992155581)), 3: ({1: 0.5096420983820886, 2: 0.27815002451380944, 3: 0.22581304134662528, 4: 0.15075992809282562}, (0.29109127308383725, 0.29109127308383725, 0.29109127308383725)), 5: ({1: 0.6170125837555156, 2: 0.34437816636705343, 3: 0.27541264912567415, 4: 0.17817453832325544}, (0.35374448439287465, 0.35374448439287465, 0.35374448439287465))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's MAX nearest retrieval stats {1: ({1: 0.3143078934466416, 2: 0.1799722176826279, 3: 0.1570518058506292, 4: 0.1153374734433731}, (0.19166734760581794, 0.19166734760581794, 0.19166734760581794)), 3: ({1: 0.5084572642588658, 2: 0.28051969276025496, 3: 0.23337146592580488, 4: 0.16309854551397288}, (0.2963617421147246, 0.2963617421147246, 0.2963617421147246)), 5: ({1: 0.6140300702729204, 2: 0.3471563981042654, 3: 0.28145938878901783, 4: 0.19226997875469848}, (0.35872895898022555, 0.35872895898022555, 0.3587289589802255))}\n",
      "train_ec=151314, test_ec=24476, train_embeddings=151314, test_embeddings=24476\n",
      "For zhihan1996/DNABERT-2-117M's AVG nearest retrieval stats {1: ({1: 0.343969602876287, 2: 0.21621179931361334, 3: 0.1943127962085308, 4: 0.14544860271286159}, (0.22498570027782316, 0.22498570027782316, 0.22498570027782316)), 3: ({1: 0.5279865991175029, 2: 0.3259519529334859, 3: 0.28427847687530644, 4: 0.21045105409380618}, (0.3371670207550253, 0.3371670207550253, 0.3371670207550253)), 5: ({1: 0.6145612028109169, 2: 0.39095440431443046, 3: 0.33620689655172414, 4: 0.245955221441412}, (0.39691943127962087, 0.39691943127962087, 0.39691943127962087))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50135aa9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ec=179587, test_ec=9715, train_embeddings=179587, test_embeddings=9715\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's class nearest retrieval stats {1: ({1: 0.4296448790530108, 2: 0.19948533196088522, 3: 0.18270715388574368, 4: 0.1054040144107051}, (0.2293103448275862, 0.2293103448275862, 0.2293103448275862)), 3: ({1: 0.573031394750386, 2: 0.2655687081832218, 3: 0.23098301595470921, 4: 0.13000514668039115}, (0.29989706639217706, 0.29989706639217706, 0.29989706639217706)), 5: ({1: 0.6545548121461657, 2: 0.31435923829130213, 3: 0.2645393721049923, 4: 0.1422542460113227}, (0.3439269171384457, 0.3439269171384457, 0.3439269171384457))}\n",
      "train_ec=179587, test_ec=9715, train_embeddings=179587, test_embeddings=9715\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's min nearest retrieval stats {1: ({1: 0.47133299022130726, 2: 0.24333504889346372, 3: 0.22460113226968606, 4: 0.13268142048378795}, (0.26798764796706126, 0.26798764796706126, 0.26798764796706126)), 3: ({1: 0.6120432321152857, 2: 0.31271230056613486, 3: 0.27596500257334017, 4: 0.16160576428203807}, (0.3405815748841997, 0.3405815748841997, 0.3405815748841997)), 5: ({1: 0.6888317035512095, 2: 0.35831188883170356, 3: 0.30777148739063304, 4: 0.1741636644364385}, (0.3822696860524961, 0.3822696860524961, 0.3822696860524961))}\n",
      "train_ec=179587, test_ec=9715, train_embeddings=179587, test_embeddings=9715\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's max nearest retrieval stats {1: ({1: 0.47833247555326813, 2: 0.242717447246526, 3: 0.22439526505404014, 4: 0.13484302624806999}, (0.2700720535254761, 0.2700720535254761, 0.2700720535254761)), 3: ({1: 0.61358723623263, 2: 0.310138960370561, 3: 0.2727740607308286, 4: 0.15872362326299536}, (0.33880597014925373, 0.33880597014925373, 0.33880597014925373)), 5: ({1: 0.6903757076685538, 2: 0.3521358723623263, 3: 0.3038600102933608, 4: 0.1724137931034483}, (0.3796963458569223, 0.3796963458569223, 0.3796963458569223))}\n",
      "train_ec=179587, test_ec=9715, train_embeddings=179587, test_embeddings=9715\n",
      "For InstaDeepAI/nucleotide-transformer-2.5b-multi-species's avg nearest retrieval stats {1: ({1: 0.4341739577972208, 2: 0.19083890890375707, 3: 0.17200205867215645, 4: 0.09243437982501286}, (0.2223623262995368, 0.2223623262995368, 0.2223623262995368)), 3: ({1: 0.584251158003088, 2: 0.2698919197117859, 3: 0.22974781266083377, 4: 0.11785898095728255}, (0.30043746783324754, 0.30043746783324754, 0.30043746783324754)), 5: ({1: 0.6659804426145136, 2: 0.3177560473494596, 3: 0.2618630983015955, 4: 0.13165208440555842}, (0.34431291816778176, 0.34431291816778176, 0.34431291816778176))}\n"
     ]
    }
   ],
   "source": [
    "train_embedding_stats = get_embedding_stats(train_embeddings)\n",
    "test_embedding_stats = get_embedding_stats(test_embeddings)\n",
    "parameters = ['class', 'min', 'max', 'avg']\n",
    "K = [1, 3, 5]\n",
    "for train_stat, test_stat, parameter in zip(train_embedding_stats, test_embedding_stats, parameters):\n",
    "    accuracy = top_k_retrieval(train_df['EC'].tolist(), test_df['EC'].tolist(), train_stat, test_stat, K)\n",
    "    print(f\"For {tokenizer.name_or_path}'s {parameter.upper()} nearest retrieval stats {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37051817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39384472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cc680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef4d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803d69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad81479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e745a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba462c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0738aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          1.0Ti        46Gi       939Gi       5.7Gi        21Gi       951Gi\n",
      "Swap:            0B          0B          0B\n",
      "Sun May 26 18:27:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              71W / 500W |  73454MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              72W / 500W |  19592MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2107477      C   ...s/eb/Anaconda3/2023.09-0/bin/python    73440MiB |\n",
      "|    1   N/A  N/A   2107477      C   ...s/eb/Anaconda3/2023.09-0/bin/python    19578MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!free -h && nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
