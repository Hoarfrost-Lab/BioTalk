{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Datasets: tremblwithsequences.csv \n",
    "# swissprotwithsequences.csv was divided in two datasets as shown in DatasetGeneration notebook : only_in_swiss.csv(This becomes part of test 2) and \n",
    "# common_in_swiss_trembl.csv(This is used as Swissprot only dataset)\n",
    "\n",
    "# For balancing the datasets we used median = 10 for Swissprot only dataset and median = 250 for Swissprot + Trembl datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark 3(Swissprot Only Unbalanced Dataset)\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.valid = pd.DataFrame(columns=df.columns)\n",
    "        self.test = pd.DataFrame(columns=df.columns)\n",
    "        self.train = pd.DataFrame(columns=df.columns)\n",
    "        self.uniref50extra = pd.DataFrame(columns=df.columns)\n",
    "        self.uniref90extra = pd.DataFrame(columns=df.columns)\n",
    "        self.extra = pd.DataFrame(columns=df.columns)\n",
    "        self.orphans = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "    def process_groups(self, df, group_by_column, extra_df):\n",
    "        for ec_number in df['EC'].unique():\n",
    "            ec_df = df[df['EC'] == ec_number]\n",
    "            grouped = ec_df.groupby(group_by_column)\n",
    "            num_groups = grouped.ngroups\n",
    "            counter = 'Test'\n",
    "            if num_groups < 3:\n",
    "                extra_df = pd.concat([extra_df, ec_df])\n",
    "                continue\n",
    "\n",
    "            if num_groups >= 10:\n",
    "                num_test = math.ceil(0.1 * num_groups)\n",
    "                num_valid = math.ceil(0.1 * num_groups)\n",
    "                num_train = num_groups - num_test - num_valid\n",
    "            elif num_groups >= 6 and num_groups <10 :\n",
    "                num_test = 2\n",
    "                num_valid = 2\n",
    "                num_train = num_groups - num_test - num_valid\n",
    "            else:\n",
    "                num_test = 1\n",
    "                num_valid = 1\n",
    "                num_train = num_groups - num_test - num_valid\n",
    "\n",
    "            for group_label, group_df in grouped:\n",
    "                if num_test > 0 and counter == 'Test':\n",
    "                    self.test = pd.concat([self.test, group_df])\n",
    "                    num_test -= 1\n",
    "                    if num_test == 0:\n",
    "                        counter = 'Valid'\n",
    "                elif num_valid > 0 and counter == 'Valid':\n",
    "                    self.valid = pd.concat([self.valid, group_df])\n",
    "                    num_valid -= 1\n",
    "                    if num_valid == 0:\n",
    "                        counter = 'Train'\n",
    "                else:\n",
    "                    self.train = pd.concat([self.train, group_df])\n",
    "\n",
    "        return extra_df\n",
    "\n",
    "    def process(self):\n",
    "        self.uniref50extra = self.process_groups(self.df, 'UniRef50', self.uniref50extra)\n",
    "        self.uniref90extra = self.process_groups(self.uniref50extra, 'UniRef90', self.uniref90extra)\n",
    "        self.extra = self.process_groups(self.uniref90extra, 'UniRef100', self.extra)\n",
    "        return self.valid, self.test, self.train, self.extra, self.orphans\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('common_in_swiss_trembl.csv')\n",
    "\n",
    "# Create an instance of the DatasetSplitter and process the data\n",
    "splitter = DatasetSplitter(df)\n",
    "valid, test, train, extra, orphans = splitter.process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a278e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the extra EC numbers\n",
    "def split_extra_by_ec(extra):\n",
    "    # Group the dataframe by 'EC'\n",
    "    df = extra.groupby('EC')\n",
    "    # Count the occurrences of each 'EC' group\n",
    "    counts_df = df.size().reset_index(name='count')\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for ec_number, group in df:\n",
    "        count = counts_df[counts_df['EC'] == ec_number]['count'].values[0]\n",
    "        if count >= 3:\n",
    "            test = pd.concat([test, group.iloc[0:1]])\n",
    "            valid = pd.concat([valid, group.iloc[1:2]])\n",
    "            train = pd.concat([train, group.iloc[2:]])\n",
    "        elif count == 2:\n",
    "            train = pd.concat([train, group.iloc[:1]])\n",
    "            test = pd.concat([test, group.iloc[1:]])\n",
    "        else:\n",
    "            orphans = pd.concat([orphans, group])\n",
    "    \n",
    "    # Save to CSV files\n",
    "    valid.to_csv('UnbalancedSwissprot/valid.csv', index=False)\n",
    "    test.to_csv('UnbalancedSwissprot/test.csv', index=False)\n",
    "    train.to_csv('UnbalancedSwissprot/train.csv', index=False)\n",
    "    orphans.to_csv('UnbalancedSwissprot/orphans.csv', index=False)\n",
    "    \n",
    "    return test, train, valid, orphans\n",
    "\n",
    "# Usage:\n",
    " test, train, valid, orphans = split_data_by_ec(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The test generated above is test set 1 or in distribution test set\n",
    "\n",
    "#Out of distribution test set (test set 2) for all benchmarks is shown below:\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv('SwissprotUnbalanced/orphans.csv')\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = pd.read_csv('only_in_swiss.csv')\n",
    "\n",
    "# Vertically concatenate the DataFrames\n",
    "combined_df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('SwissprotUnbalanced/test2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77192a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark 4(Swissprot Only Balanced Dataset)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def balance_data_by_ec(input_file, output_file):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Group by EC number\n",
    "    grouped = df.groupby('EC')\n",
    "\n",
    "    # Initialize empty DataFrame for sampled data\n",
    "    sampled_data = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Iterate over groups\n",
    "    for name, group in grouped:\n",
    "        num_records = len(group)\n",
    "        \n",
    "        if num_records < 10:\n",
    "            # Make copies of available records until there are 10 records\n",
    "            copies_needed = 10 - num_records\n",
    "            copies = group.sample(copies_needed, replace=True)\n",
    "            sampled_data = pd.concat([sampled_data, group, copies])\n",
    "        elif num_records == 10:\n",
    "            sampled_data = pd.concat([sampled_data, group])\n",
    "        else:\n",
    "            # Randomly sample 10 records ensuring they belong to different uniref50 clusters\n",
    "            sampled_group = pd.DataFrame(columns=df.columns)\n",
    "            unique_clusters = group['UniRef50'].unique()\n",
    "            if len(unique_clusters) < 10:\n",
    "                # If there are less than 10 clusters, sample different records from available clusters\n",
    "                for cluster in unique_clusters:\n",
    "                    cluster_records = group[group['UniRef50'] == cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "                # Sample remaining records to make up 10\n",
    "                remaining_clusters = 10 - len(unique_clusters)\n",
    "                for _ in range(remaining_clusters):\n",
    "                    random_cluster = random.choice(unique_clusters)\n",
    "                    cluster_records = group[group['UniRef50'] == random_cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "            else:\n",
    "                sampled_clusters = random.sample(list(unique_clusters), 10)\n",
    "                for cluster in sampled_clusters:\n",
    "                    cluster_records = group[group['UniRef50'] == cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "            sampled_data = pd.concat([sampled_data, sampled_group])        \n",
    "        \n",
    "    # Write sampled data to CSV\n",
    "    sampled_data.to_csv(output_file, index=False)\n",
    "\n",
    "# Usage\n",
    "balance_data_by_ec('train.csv', 'BalancedSwissprot/train.csv')\n",
    "balance_data_by_ec('valid.csv', 'BalancedSwissprot/valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8518ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark 1 (Swissprot + Trembl combined, Unbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mixing both datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Read the UniProt and SwissProt DataFrames from CSV files\n",
    "trembl = pd.read_csv(\"./Trembl.csv\")\n",
    "swiss = pd.read_csv(\"./Swissprot.csv\")\n",
    "\n",
    "# Add 'DBtype' column with respective values\n",
    "trembl['DBtype'] = 'Uniprot'\n",
    "swiss['DBtype'] = 'Swissprot'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "swiss_and_uni = pd.concat([trembl, swiss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.valid = pd.DataFrame(columns=df.columns)\n",
    "        self.train = pd.DataFrame(columns=df.columns)\n",
    "        self.uniref50extra = pd.DataFrame(columns=df.columns)\n",
    "        self.uniref90extra = pd.DataFrame(columns=df.columns)\n",
    "        self.extra = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    def process_groups(self, df, group_by_column, extra_df):\n",
    "        for ec_number in df['EC'].unique():\n",
    "            ec_df = df[df['EC'] == ec_number]\n",
    "            grouped = ec_df.groupby(group_by_column)\n",
    "            num_groups = grouped.ngroups\n",
    "            counter = 'Valid'\n",
    "            \n",
    "            if num_groups < 2:\n",
    "                extra_df = pd.concat([extra_df, ec_df])\n",
    "                continue\n",
    "            \n",
    "            if num_groups >= 10:\n",
    "                num_valid = math.ceil(0.1 * num_groups)\n",
    "                num_train = num_groups - num_valid\n",
    "            elif num_groups >= 6 and num_groups < 10:\n",
    "                num_valid = 2\n",
    "                num_train = num_groups - num_valid\n",
    "            else:\n",
    "                num_valid = 1\n",
    "                num_train = num_groups - num_valid\n",
    "\n",
    "            for group_label, group_df in grouped:\n",
    "                if num_valid > 0 and counter == 'Valid':\n",
    "                    self.valid = pd.concat([self.valid, group_df])\n",
    "                    num_valid -= 1\n",
    "                    if num_valid == 0:\n",
    "                        counter = 'Train'\n",
    "                else:\n",
    "                    self.train = pd.concat([self.train, group_df])\n",
    "\n",
    "        return extra_df\n",
    "\n",
    "    def process(self):\n",
    "        self.uniref50extra = self.process_groups(self.df, 'UniRef50', self.uniref50extra)\n",
    "        self.uniref90extra = self.process_groups(self.uniref50extra, 'UniRef90', self.uniref90extra)\n",
    "        self.extra = self.process_groups(self.uniref90extra, 'UniRef100', self.extra)\n",
    "        return self.valid, self.train, self.extra\n",
    "\n",
    "# Read the dataset\n",
    "input_file = 'path_to_your_input_file.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create an instance of the DatasetSplitter and process the data\n",
    "splitter = DatasetSplitter(df)\n",
    "valid, train, extra = splitter.process()\n",
    "\n",
    "train = pd.concat([train,extra])\n",
    "\n",
    "# Save the results\n",
    "valid.to_csv('UnbalancedUniprot/valid.csv', index=False)\n",
    "train.to_csv('UnbalancedUniprot/train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark 2 (Swissprot + Trembl combined, Balanced)\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def balance_data_by_ec(input_file, output_file):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Group by EC number\n",
    "    grouped = df.groupby('EC')\n",
    "\n",
    "    # Initialize empty DataFrame for sampled data\n",
    "    sampled_data = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Iterate over groups\n",
    "    for name, group in grouped:\n",
    "        num_records = len(group)\n",
    "        \n",
    "        if num_records < 250:\n",
    "            # Make copies of available records until there are 10 records\n",
    "            copies_needed = 250 - num_records\n",
    "            copies = group.sample(copies_needed, replace=True)\n",
    "            sampled_data = pd.concat([sampled_data, group, copies])\n",
    "        elif num_records == 250:\n",
    "            sampled_data = pd.concat([sampled_data, group])\n",
    "        else:\n",
    "            # Randomly sample 250 records ensuring they belong to different uniref50 clusters\n",
    "            sampled_group = pd.DataFrame(columns=df.columns)\n",
    "            unique_clusters = group['UniRef50'].unique()\n",
    "            if len(unique_clusters) < 250:\n",
    "                # If there are less than 10 clusters, sample different records from available clusters\n",
    "                for cluster in unique_clusters:\n",
    "                    cluster_records = group[group['UniRef50'] == cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "                # Sample remaining records to make up 250\n",
    "                remaining_clusters = 250 - len(unique_clusters)\n",
    "                for _ in range(remaining_clusters):\n",
    "                    random_cluster = random.choice(unique_clusters)\n",
    "                    cluster_records = group[group['UniRef50'] == random_cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "            else:\n",
    "                sampled_clusters = random.sample(list(unique_clusters), 250)\n",
    "                for cluster in sampled_clusters:\n",
    "                    cluster_records = group[group['UniRef50'] == cluster]\n",
    "                    sampled_record = cluster_records.sample(n=1)\n",
    "                    sampled_group = pd.concat([sampled_group, sampled_record])\n",
    "            sampled_data = pd.concat([sampled_data, sampled_group])        \n",
    "        \n",
    "    # Write sampled data to CSV\n",
    "    sampled_data.to_csv(output_file, index=False)\n",
    "\n",
    "# Usage\n",
    "balance_data_by_ec('train.csv', 'BalancedUniprot/train.csv')\n",
    "balance_data_by_ec('valid.csv', 'BalancedUniprot/valid.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
