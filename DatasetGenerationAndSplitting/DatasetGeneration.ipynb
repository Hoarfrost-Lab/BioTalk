{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and unzip ID mapping file : \n",
    "\n",
    "wget -b https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz\n",
    "\n",
    "gunzip -c idmapping_selected.tab.gz > idmapping_selected.tab &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Uniprot TreEbml and Swissprot \n",
    "\n",
    "wget -b https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.tab.gz\n",
    "\n",
    "gunzip -c uniport_trembl.tab.gz > uniport_trembl.tab &\n",
    "\n",
    "\n",
    "wget -b https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.tab.gz\n",
    "\n",
    "gunzip -c uniport_swissprot.tab.gz > uniport_swissprot.tab &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Accession ID, Uniref50 ID, Uniref90 ID, Uniref100 ID and EmblCdsId from idmapping file\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def extract_id(row, keyword):\n",
    "    for element in row:\n",
    "        if keyword in element:\n",
    "            return element\n",
    "    return None\n",
    "\n",
    "def extract_ids_from_file(input_file, output_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            csv_reader = csv.reader(file, delimiter='\\t')\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "\n",
    "            # Writing header\n",
    "            csv_writer.writerow(['AC', 'UniRef100', 'UniRef90', 'UniRef50', 'EmblCdsId'])\n",
    "\n",
    "            # Process each row\n",
    "            for row in csv_reader:\n",
    "                # Extracting the IDs based on keywords\n",
    "                uniref100 = extract_id(row, 'UniRef100')\n",
    "                uniref90 = extract_id(row, 'UniRef90')\n",
    "                uniref50 = extract_id(row, 'UniRef50')\n",
    "                \n",
    "                emblcds = row[-1]\n",
    "                ac = row[0]\n",
    "\n",
    "                # Writing data\n",
    "                csv_writer.writerow([ac, uniref100, uniref90, uniref50, emblcds])\n",
    "\n",
    "    print(f\"Extracted IDs written to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "input_file = 'idmapping_selected.tab'\n",
    "output_file = 'idmapping.csv'\n",
    "extract_ids_from_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract EC number(EC) and Accession ID(AC) for Bacteria & Archaea (OC)\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_ac_ec_oc(input_file_path, output_file_path):\n",
    "    \n",
    "    # Open the input TSV file for reading\n",
    "    with open(input_file_path, 'r', newline='') as file:\n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['AC', 'EC', 'OC'])  # Write the header row to the CSV file\n",
    "\n",
    "            for row in csv.reader(file, delimiter='\\t'):\n",
    "                superkingdom = ''  # Default value if no match is found in the line\n",
    "\n",
    "                # Extract superkingdom from the taxonomic lineage\n",
    "                superkingdom_match = re.search(r'(Bacteria|Archaea)', row[2])\n",
    "                if superkingdom_match:\n",
    "                    superkingdom = superkingdom_match.group(1)\n",
    "                    entry = row[0]\n",
    "                    ec_number = row[1]\n",
    "                    writer.writerow([entry, ec_number, superkingdom])\n",
    "\n",
    "# Extract Values from uniprot trembl \n",
    "extract_ac_ec_oc('uniport_trembl.tab', 'extracted_trembl.csv')\n",
    "# Extract Values from uniprot swissprot \n",
    "extract_ac_ec_oc('uniport_swissprot.tab', 'extracted_swissprot.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with blank EC values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def drop_blank_entries(input_csv, output_csv):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Drop rows with any empty cells\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Save the cleaned DataFrame to a new CSV file\n",
    "        df.to_csv(output_csv, index=False)\n",
    "\n",
    "        print(\"Blank entries dropped and saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Usage for treml.csv\n",
    "input_csv_path = \"extracted_trembl.csv\"\n",
    "output_csv_path = \"trembl.csv\"\n",
    "drop_blank_entries(input_csv_path, output_csv_path)\n",
    "\n",
    "# Usage for swissprot.csv\n",
    "input_csv_path = \"extracted_swissprot.csv\"\n",
    "output_csv_path = \"swissprot.csv\"\n",
    "drop_blank_entries(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing records where EC Number contains - (example : 1.2.1.-,1.-.-.-)\n",
    "\n",
    "import csv\n",
    "\n",
    "def remove_ec_numbers_with_dash(input_file, output_file):\n",
    "    # Define a function to check if EC number contains any '-'\n",
    "    def has_dash(ec_number):\n",
    "        return '-' in ec_number\n",
    "\n",
    "    # Open input and output files\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\", newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Write the header to the output file\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Process each line in the input file\n",
    "        for row in reader:\n",
    "            # Check if EC number has '-'\n",
    "            if not has_dash(row[1]):  # Assuming EC number is in the second column (index 1)\n",
    "                # Write the line to the output file\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Usage\n",
    "remove_ec_numbers_with_dash(\"trembl.csv\", \"cleaned_trembl.csv\")\n",
    "remove_ec_numbers_with_dash(\"swissprot.csv\", \"cleaned_swissprot.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e832eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging idmapping file and Uniprot files\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(file1, file2, output_file):\n",
    "\n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Merge the DataFrames on the 'AC' column\n",
    "    merged_df = pd.merge(df1, df2, on='AC')\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"Files have been successfully merged and saved to\", output_file)\n",
    "\n",
    "# Usage\n",
    "file1 = './cleaned_trembl.csv'\n",
    "file2 = './idmapping.csv'\n",
    "output_file = './mapped_trembl.csv'\n",
    "merge_csv_files(file1, file2, output_file)\n",
    "\n",
    "file1 = './cleaned_swissprot.csv'\n",
    "file2 = './idmapping.csv'\n",
    "output_file = './mapped_swissprot.csv'\n",
    "merge_csv_files(file1, file2, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the records which contain more than one embl cds id in one field, split that entry into multiple rows, each row should contain one embl cds id\n",
    "import pandas as pd\n",
    "\n",
    "def split_embl_cds_id(file):\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Create a new DataFrame to store the split EmblCdsId values\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        embl_ids = str(row['EmblCdsId']).split(';')  # Convert to string and then split by semicolon\n",
    "        for embl_id in embl_ids:\n",
    "            # Check if the value contains a hyphen\n",
    "            if '-' not in embl_id:\n",
    "                new_row = row.copy()\n",
    "                new_row['EmblCdsId'] = embl_id\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Update the original file with the new data\n",
    "    new_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Usage:\n",
    "file = './mapped_trembl.csv'\n",
    "split_embl_cds_id(file)\n",
    "\n",
    "file = './mapped_swissprot.csv'\n",
    "split_embl_cds_id(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146df6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the records which contain more than one EC number in one field, split that entry into multiple rows, each row should contain one EC Number\n",
    "import pandas as pd\n",
    "\n",
    "def split_ec_number(file):\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Create a new DataFrame to store the split EmblCdsId values\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        ec_numbers = str(row['EC']).split(';')  # Convert to string and then split by semicolon\n",
    "        for ec_number in ec_numbers:\n",
    "            new_row = row.copy()\n",
    "            new_row['EC'] = ec_number\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Update the original file with the new data\n",
    "    new_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Usage:\n",
    "file = './mapped_trembl.csv'\n",
    "split_ec_number(file)\n",
    "\n",
    "file = './mapped_swissprot.csv'\n",
    "split_ec_number(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download sequences for each embl cds id\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import certifi\n",
    "import contextlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def download_batch(ids):\n",
    "    url_root = 'https://www.ebi.ac.uk/ena/browser/api/fasta/'\n",
    "    url = url_root + ','.join(ids)\n",
    "    try:\n",
    "        response = requests.get(url, verify=certifi.where()).text\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request for {','.join(ids)}: {e}\")\n",
    "        return \"\"\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_seqs(ids, output_file):\n",
    "    batch_size = 500  # Adjust batch size as needed\n",
    "    starts = list(np.arange(0, len(ids), batch_size))\n",
    "    stops = list(np.arange(batch_size, len(ids), batch_size)) + [len(ids)]\n",
    "\n",
    "    with open(output_file, 'a') as f:\n",
    "        with contextlib.closing(f):\n",
    "            with Pool(processes=4) as pool:  # Adjust the number of processes as needed\n",
    "                results = list(tqdm(pool.imap(download_batch, [ids[start:stop] for start, stop in zip(starts, stops)]), total=len(starts)))\n",
    "\n",
    "            for result in results:\n",
    "                f.write(result)\n",
    "\n",
    "# Usage\n",
    "cds_id = pd.read_csv('mapped_trembl.csv')\n",
    "get_seqs(cds_id['EmblCdsId'].str.replace(' ', ''), output_file= 'trembl_sequences.fasta')\n",
    "\n",
    "cds_id = pd.read_csv('mapped_swissprot.csv')\n",
    "get_seqs(cds_id['EmblCdsId'].str.replace(' ', ''), output_file= 'swissprot_sequences.fasta')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract embl cds id and sequences from the downloaded fasta files. Create a final file which contains the colummns:\n",
    "AC, EC, OC, UniRef50, UniRef90, UniRef100, EmblCdsId, Seq \"\"\"\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "\n",
    "def extract_embl_cds_id(header):\n",
    "    match = re.search(r'\\|([^|]+)\\|([^\\s]+)', header)\n",
    "    return match.group(2) if match else None\n",
    "\n",
    "def retrieve_embl_cds_id_and_sequence(csv_file, fasta_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df_csv = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Create a dictionary to store EmblCdsId as keys and sequences as values\n",
    "    sequence_dict = {}\n",
    "\n",
    "    # Read the FASTA file and populate the dictionary\n",
    "    for record in SeqIO.parse(fasta_file, 'fasta'):\n",
    "        header = record.description\n",
    "        embl_cds_id = extract_embl_cds_id(header)\n",
    "        if embl_cds_id:\n",
    "            sequence_dict[embl_cds_id] = str(record.seq)\n",
    "\n",
    "    # Check if any EmblCdsId from the CSV file matches those in the FASTA dictionary\n",
    "    matching_embl_cds_ids = set(df_csv['EmblCdsId']).intersection(sequence_dict.keys())\n",
    "\n",
    "    if not matching_embl_cds_ids:\n",
    "        print(\"No matching EmblCdsIds found.\")\n",
    "    else:\n",
    "        # Add the 'Sequence' column based on the dictionary\n",
    "        df_csv['Sequence'] = df_csv['EmblCdsId'].map(sequence_dict)\n",
    "\n",
    "        # Filter the DataFrame based on matching EmblCdsIds\n",
    "        df_merged = df_csv[df_csv['EmblCdsId'].isin(matching_embl_cds_ids)]\n",
    "\n",
    "        # Rearrange columns with 'EmblCdsId' as the last column\n",
    "        df_merged = df_merged[['AC','EC','OC','UniRef100','UniRef90','UniRef50','EmblCdsId','Sequence']]\n",
    "\n",
    "        # Save the merged data to a new CSV file\n",
    "        df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# Usage\n",
    "retrieve_embl_cds_id_and_sequence('mapped_trembl.csv', 'trembl_sequences.fasta', 'tremblwithsequences.csv')\n",
    "\n",
    "retrieve_embl_cds_id_and_sequence('mapped_swissprot.csv', 'swissprot_sequences.fasta', 'swissprotwithsequences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4943f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING SOME ANALYSIS BEFORE SPLITTING THE DATASETS AND CREATING BENCHMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9eaf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of records for each EC number\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_ec_counts(input_file, output_file, chunk_size):\n",
    "    # Initialize a counter dictionary to store counts of EC values\n",
    "    ec_counts = {}\n",
    "\n",
    "    # Iterate over chunks of the file\n",
    "    for chunk in pd.read_csv(input_file, chunk_size):\n",
    "        # Count the occurrences of each EC value in the current chunk\n",
    "        chunk_ec_counts = chunk['EC'].value_counts().to_dict()\n",
    "        \n",
    "        # Update the overall count dictionary with counts from the current chunk\n",
    "        for ec, count in chunk_ec_counts.items():\n",
    "            ec_counts[ec] = ec_counts.get(ec, 0) + count\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    ec_counts_df = pd.DataFrame(list(ec_counts.items()), columns=['EC', 'Count'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    ec_counts_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Usage:\n",
    "calculate_ec_counts('tremblwithsequences.csv', 'trembl_ec_count.csv', 100000)\n",
    "calculate_ec_counts('swissprotwithsequences.csv', 'swissprot_ec_count.csv', 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniprot Trembl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "trembl_df = pd.read_csv('tremblwithsequences.csv')\n",
    "swissprot_df = pd.read_csv('swissprotwithsequences.csv')\n",
    "\n",
    "ec_trembl = set(trembl_df['EC'])\n",
    "ec_swissprot = set(swissprot_df['EC'])\n",
    "\n",
    "# Find the differences\n",
    "ec_numbers_only_in_trembl = ec_trembl - ec_swissprot\n",
    "print(len(ec_numbers_only_in_trembl))\n",
    "\n",
    "ec_numbers_only_in_swissprot = ec_swissprot - ec_trembl\n",
    "print(len(ec_numbers_only_in_swissprot))\n",
    "\n",
    "\n",
    "# Print the EC numbers with counts from file 1\n",
    "print(\"EC numbers only in file 1 with counts:\")\n",
    "for ec_number in ec_numbers_only_in_trembl:\n",
    "    count = ec_trembl[ec_trembl['EC'] == ec_number]['Count'].iloc[0]  # Assuming 'count' is the column name in file1.csv\n",
    "    print(f\"EC number: {ec_number}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EC numbers only in file 2 with counts:\")\n",
    "for ec_number in ec_numbers_only_in_swissprot:\n",
    "    count = ec_swissprot[ec_swissprot['EC'] == ec_number]['Count'].iloc[0]  # Assuming 'count' is the column name in file1.csv\n",
    "    print(f\"EC number: {ec_number}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate median for trembl\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "\n",
    "sorted_trembl = trembl_df.sort_values('Count')\n",
    "\n",
    "display(sorted_trembl.style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_trembl['Count'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding median for Swissprot after removing ec number only present in Swiss\n",
    "\n",
    "common_trembl_swiss = ec_swissprot - ec_numbers_only_in_swissprot\n",
    "\n",
    "print(len(common_trembl_swiss))\n",
    "\n",
    "# Print the EC numbers with counts from file 2\n",
    "print(\"EC numbers common in swiss and uni with counts:\")\n",
    "for ec_number in common_trembl_swiss:\n",
    "    count = df2[df2['EC'] == ec_number]['Count'].iloc[0]  # Assuming 'count' is the column name in file1.csv\n",
    "    print(f\"EC number: {ec_number}, Count: {count}\")\n",
    "    sum += count\n",
    "    \n",
    "print(sum)\n",
    "\n",
    "# sorted_common_uni_swiss = common_uni_swiss.sort_values('Count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#swissprot ALL \n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "\n",
    "sorted_swissprot = swissprot_df.sort_values('Count')\n",
    "\n",
    "display(sorted_swissprot.style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b72802",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df2[df2['EC'].isin(common_uni_swiss)]\n",
    "\n",
    "len(filtered_df)\n",
    "sorted = filtered_df.sort_values('Count')\n",
    "sorted['Count'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating records which were only in Swiss not Uni\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file1_path = '.csv'\n",
    "file1_df = pd.read_csv(file1_path)\n",
    "\n",
    "# Assuming you have a set 'ec_numbers_only_in_file2' containing EC numbers\n",
    "# Extract EC numbers from set 'ec_numbers_only_in_file2'\n",
    "ec_numbers = ec_numbers_only_in_file2\n",
    "\n",
    "# Filter file1_df to get rows where 'EC' column contains EC numbers from 'ec_numbers'\n",
    "filtered_df = file1_df[file1_df['EC'].isin(ec_numbers)]\n",
    "\n",
    "# Get the remaining data\n",
    "remaining_df = file1_df[~file1_df['EC'].isin(ec_numbers)]\n",
    "\n",
    "# Save the filtered data to a new file\n",
    "filtered_file_path = 'only_in_swiss.csv'\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "# Save the remaining data to another file\n",
    "remaining_file_path = 'common_in_swiss_trembl.csv'\n",
    "remaining_df.to_csv(remaining_file_path, index=False)\n",
    "\n",
    "print(\"Filtered data and remaining data have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Datasets: tremblwithsequences.csv \n",
    "# tremblwithsequences.csv was divided in two datasets as shown above : only_in_swiss.csv(This becomes part of test 2) and \n",
    "# common_in_swiss_trembl.csv(This is used as Swissprot only dataset)\n",
    "\n",
    "# For balancing the datasets we used median = 10 for Swissprot only dataset and median = 250 for Swissprot + Trembl datset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
